{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2068df6-702e-43fc-85e1-431884953a92",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"LMS Logo (1).png\" alt=\"TIET Logo\" style=\"width: 200px;\">\n",
    "\n",
    "#### *Thapar Institute of Engineering and Technology, Patiala <br> Computer Science & Engineering Department*  \n",
    "\n",
    "#### ***UCS420: Cognitive Computing <br> <br> Lab Assignment-9 <br> Natural Language Processing using Python - I***\n",
    "\n",
    "#### *Submitted by Smarth Kaushal [Roll No. 102497023] to Dr. Vaishali*\n",
    "---\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f435cde-d6d1-41af-b9cf-e46c5628510e",
   "metadata": {},
   "source": [
    "&nbsp;**I.     Text Preprocessing:**\n",
    "\n",
    "Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.) and perform the following tasks.     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. Convert text to lowercase and remove punctuation.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. Tokenize the text into words and sentences.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Remove stopwords (using NLTK's stopwords list).   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4. Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f627b694-4ce7-49b9-9fed-5058d85db82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.stem     import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6957eee8-6e90-4f31-ba95-16be4ae4ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 4th semester in a CSE student’s journey is where things get real — a transition from basic programming to core subjects like DBMS, Operating Systems, Data Structures and Algorithms and Machine Learning.\n",
       "Assignments pile up, coding contests become routine, and group projects start resembling mini-startups.\n",
       "Balancing academics with internships, DSA prep, and side projects feels like a juggle, but amidst all this, we often find that one true friend—the person who pulls us through debugging nightmares, shares midnight Maggi, and makes deadlines bearable.\n",
       "And sometimes, a new kind of logical error sneaks in — the other-side crush, where a silent admiration in the library or casual eye contact in the cafeteria adds an unexpected twist.\n",
       "But through it all, friendships, chai breaks, and hostel banter keep us sane, making this phase a rollercoaster of code, career choices, and heartbeats—testing both patience and multitasking in ways we never imagined!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define the paragraph\n",
    "mytxt = \"\"\"The 4th semester in a CSE student’s journey is where things get real — a transition from basic programming to core subjects like DBMS, Operating Systems, Data Structures and Algorithms and Machine Learning.\n",
    "Assignments pile up, coding contests become routine, and group projects start resembling mini-startups.\n",
    "Balancing academics with internships, DSA prep, and side projects feels like a juggle, but amidst all this, we often find that one true friend—the person who pulls us through debugging nightmares, shares midnight Maggi, and makes deadlines bearable.\n",
    "And sometimes, a new kind of logical error sneaks in — the other-side crush, where a silent admiration in the library or casual eye contact in the cafeteria adds an unexpected twist.\n",
    "But through it all, friendships, chai breaks, and hostel banter keep us sane, making this phase a rollercoaster of code, career choices, and heartbeats—testing both patience and multitasking in ways we never imagined!\"\"\"\n",
    "\n",
    "# Split into lines\n",
    "lines = mytxt.split(\"\\n\")\n",
    "\n",
    "# Display formatted text\n",
    "display(Markdown(mytxt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60e71e9-0f64-407f-99f7-3c7142900ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['4th', 'semester', 'cse', 'student', '’', 'journey', 'things', 'get', 'real', '—', 'transition', 'basic', 'programming', 'core', 'subjects', 'like', 'dbms', 'operating', 'systems', 'data', 'structures', 'algorithms', 'machine', 'learning', 'assignments', 'pile', 'coding', 'contests', 'become', 'routine', 'group', 'projects', 'start', 'resembling', 'ministartups', 'balancing', 'academics', 'internships', 'dsa', 'prep', 'side', 'projects', 'feels', 'like', 'juggle', 'amidst', 'often', 'find', 'one', 'true', 'friend—the', 'person', 'pulls', 'us', 'debugging', 'nightmares', 'shares', 'midnight', 'maggi', 'makes', 'deadlines', 'bearable', 'sometimes', 'new', 'kind', 'logical', 'error', 'sneaks', '—', 'otherside', 'crush', 'silent', 'admiration', 'library', 'casual', 'eye', 'contact', 'cafeteria', 'adds', 'unexpected', 'twist', 'friendships', 'chai', 'breaks', 'hostel', 'banter', 'keep', 'us', 'sane', 'making', 'phase', 'rollercoaster', 'code', 'career', 'choices', 'heartbeats—testing', 'patience', 'multitasking', 'ways', 'never', 'imagined']\n",
      "Word Frequencies:\n",
      "FreqDist({'—': 2, 'like': 2, 'projects': 2, 'us': 2, '4th': 1, 'semester': 1, 'cse': 1, 'student': 1, '’': 1, 'journey': 1, ...})\n"
     ]
    }
   ],
   "source": [
    "# Step 1:Convert to lowercase and remove punctuation\n",
    "text = mytxt.lower()\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Step 2: Tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "# Step 3: Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "# Step 4: Frequency Distribution\n",
    "fd = FreqDist(filtered_words)\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "print(\"Word Frequencies:\")\n",
    "fd.pprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e8992-0c69-4c04-97ee-027c4aa8a46b",
   "metadata": {},
   "source": [
    "**II. Stemming and Lemmatization:**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. Take the tokenized words from I part (after stopword removal).  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Apply lemmatization using NLTK's WordNetLemmatizer.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4. Compare and display results of both techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3016d768-d1c8-489c-a4bd-28941dda4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['4th', 'semest', 'cse', 'student', '’', 'journey', 'thing', 'get', 'real', '—', 'transit', 'basic', 'program', 'core', 'subject', 'like', 'dbm', 'oper', 'system', 'data', 'structur', 'algorithm', 'machin', 'learn', 'assign', 'pile', 'code', 'contest', 'becom', 'routin', 'group', 'project', 'start', 'resembl', 'ministartup', 'balanc', 'academ', 'internship', 'dsa', 'prep', 'side', 'project', 'feel', 'like', 'juggl', 'amidst', 'often', 'find', 'one', 'true', 'friend—th', 'person', 'pull', 'us', 'debug', 'nightmar', 'share', 'midnight', 'maggi', 'make', 'deadlin', 'bearabl', 'sometim', 'new', 'kind', 'logic', 'error', 'sneak', '—', 'othersid', 'crush', 'silent', 'admir', 'librari', 'casual', 'eye', 'contact', 'cafeteria', 'add', 'unexpect', 'twist', 'friendship', 'chai', 'break', 'hostel', 'banter', 'keep', 'us', 'sane', 'make', 'phase', 'rollercoast', 'code', 'career', 'choic', 'heartbeats—test', 'patienc', 'multitask', 'way', 'never', 'imagin']\n",
      "\n",
      "Lancaster Stemmer: ['4th', 'semest', 'cse', 'stud', '’', 'journey', 'thing', 'get', 'real', '—', 'transit', 'bas', 'program', 'cor', 'subject', 'lik', 'dbms', 'op', 'system', 'dat', 'structures', 'algorithm', 'machin', 'learn', 'assign', 'pil', 'cod', 'contest', 'becom', 'routin', 'group', 'project', 'start', 'resembl', 'ministartup', 'bal', 'academ', 'intern', 'dsa', 'prep', 'sid', 'project', 'feel', 'lik', 'juggl', 'amidst', 'oft', 'find', 'on', 'tru', 'friend—the', 'person', 'pul', 'us', 'debug', 'nightm', 'shar', 'midnight', 'magg', 'mak', 'deadlin', 'bear', 'sometim', 'new', 'kind', 'log', 'er', 'sneak', '—', 'othersid', 'crush', 'sil', 'admir', 'libr', 'cas', 'ey', 'contact', 'cafeter', 'ad', 'unexpect', 'twist', 'friend', 'cha', 'break', 'hostel', 'bant', 'keep', 'us', 'san', 'mak', 'phas', 'rollercoast', 'cod', 'car', 'cho', 'heartbeats—testing', 'paty', 'multitask', 'way', 'nev', 'imagin']\n",
      "\n",
      "Lemmatized: ['4th', 'semester', 'cse', 'student', '’', 'journey', 'thing', 'get', 'real', '—', 'transition', 'basic', 'programming', 'core', 'subject', 'like', 'dbms', 'operating', 'system', 'data', 'structure', 'algorithm', 'machine', 'learning', 'assignment', 'pile', 'coding', 'contest', 'become', 'routine', 'group', 'project', 'start', 'resembling', 'ministartups', 'balancing', 'academic', 'internship', 'dsa', 'prep', 'side', 'project', 'feel', 'like', 'juggle', 'amidst', 'often', 'find', 'one', 'true', 'friend—the', 'person', 'pull', 'u', 'debugging', 'nightmare', 'share', 'midnight', 'maggi', 'make', 'deadline', 'bearable', 'sometimes', 'new', 'kind', 'logical', 'error', 'sneak', '—', 'otherside', 'crush', 'silent', 'admiration', 'library', 'casual', 'eye', 'contact', 'cafeteria', 'add', 'unexpected', 'twist', 'friendship', 'chai', 'break', 'hostel', 'banter', 'keep', 'u', 'sane', 'making', 'phase', 'rollercoaster', 'code', 'career', 'choice', 'heartbeats—testing', 'patience', 'multitasking', 'way', 'never', 'imagined']\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Use filtered_words from Q1\n",
    "porter_result = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_result = [lancaster.stem(word) for word in filtered_words]\n",
    "lemma_result = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Porter Stemmer:\", porter_result)\n",
    "print(\"\\nLancaster Stemmer:\", lancaster_result)\n",
    "print(\"\\nLemmatized:\", lemma_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4a22c-c791-408a-b9fe-68bdecd80461",
   "metadata": {},
   "source": [
    "**III. Regular Expressions and Text Splitting:**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. Take the original text from I.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. Use regular expressions to:     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) Extract all words with more than 5 letters.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) Extract all numbers (if any exist in their text).   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) Extract all capitalized words.    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Use text splitting techniques to:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) Split the text into words containing only alphabets (removing digits and special characters).   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) Extract words starting with a vowel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a734889-641d-414c-a0a8-7f4cf1689b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words > 5 letters: ['semester', 'student', 'journey', 'things', 'transition', 'programming', 'subjects', 'Operating', 'Systems', 'Structures', 'Algorithms', 'Machine', 'Learning', 'Assignments', 'coding', 'contests', 'become', 'routine', 'projects', 'resembling', 'startups', 'Balancing', 'academics', 'internships', 'projects', 'juggle', 'amidst', 'friend', 'person', 'through', 'debugging', 'nightmares', 'shares', 'midnight', 'deadlines', 'bearable', 'sometimes', 'logical', 'sneaks', 'silent', 'admiration', 'library', 'casual', 'contact', 'cafeteria', 'unexpected', 'through', 'friendships', 'breaks', 'hostel', 'banter', 'making', 'rollercoaster', 'career', 'choices', 'heartbeats', 'testing', 'patience', 'multitasking', 'imagined']\n",
      "\n",
      "Numbers: ['4']\n",
      "\n",
      "Capitalized words: ['The', 'Operating', 'Systems', 'Data', 'Structures', 'Algorithms', 'Machine', 'Learning', 'Assignments', 'Balancing', 'Maggi', 'And', 'But']\n",
      "\n",
      "Alphabet-only words: ['The', 'semester', 'in', 'a', 'CSE', 'student', 's', 'journey', 'is', 'where', 'things', 'get', 'real', 'a', 'transition', 'from', 'basic', 'programming', 'to', 'core', 'subjects', 'like', 'DBMS', 'Operating', 'Systems', 'Data', 'Structures', 'and', 'Algorithms', 'and', 'Machine', 'Learning', 'Assignments', 'pile', 'up', 'coding', 'contests', 'become', 'routine', 'and', 'group', 'projects', 'start', 'resembling', 'mini', 'startups', 'Balancing', 'academics', 'with', 'internships', 'DSA', 'prep', 'and', 'side', 'projects', 'feels', 'like', 'a', 'juggle', 'but', 'amidst', 'all', 'this', 'we', 'often', 'find', 'that', 'one', 'true', 'friend', 'the', 'person', 'who', 'pulls', 'us', 'through', 'debugging', 'nightmares', 'shares', 'midnight', 'Maggi', 'and', 'makes', 'deadlines', 'bearable', 'And', 'sometimes', 'a', 'new', 'kind', 'of', 'logical', 'error', 'sneaks', 'in', 'the', 'other', 'side', 'crush', 'where', 'a', 'silent', 'admiration', 'in', 'the', 'library', 'or', 'casual', 'eye', 'contact', 'in', 'the', 'cafeteria', 'adds', 'an', 'unexpected', 'twist', 'But', 'through', 'it', 'all', 'friendships', 'chai', 'breaks', 'and', 'hostel', 'banter', 'keep', 'us', 'sane', 'making', 'this', 'phase', 'a', 'rollercoaster', 'of', 'code', 'career', 'choices', 'and', 'heartbeats', 'testing', 'both', 'patience', 'and', 'multitasking', 'in', 'ways', 'we', 'never', 'imagined']\n",
      "\n",
      "Words starting with vowels: ['in', 'a', 'is', 'a', 'Operating', 'and', 'Algorithms', 'and', 'Assignments', 'up', 'and', 'academics', 'internships', 'and', 'a', 'amidst', 'all', 'often', 'one', 'us', 'and', 'And', 'a', 'of', 'error', 'in', 'other', 'a', 'admiration', 'in', 'or', 'eye', 'in', 'adds', 'an', 'unexpected', 'it', 'all', 'and', 'us', 'a', 'of', 'and', 'and', 'in', 'imagined']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original text before preprocessing\n",
    "original_text = mytxt\n",
    "\n",
    "# a. Words with more than 5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', original_text)\n",
    "\n",
    "# b. Numbers (if any)\n",
    "numbers = re.findall(r'\\d+', original_text)\n",
    "\n",
    "# c. Capitalized words\n",
    "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', original_text)\n",
    "\n",
    "# d. Words with only alphabets\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', original_text)\n",
    "\n",
    "# e. Words starting with a vowel\n",
    "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']\n",
    "\n",
    "print(\"Words > 5 letters:\", long_words)\n",
    "print(\"\\nNumbers:\", numbers)\n",
    "print(\"\\nCapitalized words:\", capitalized)\n",
    "print(\"\\nAlphabet-only words:\", alpha_words)\n",
    "print(\"\\nWords starting with vowels:\", vowel_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d165d-c759-453e-a8c6-8ca723496e46",
   "metadata": {},
   "source": [
    "**IV. Custom Tokenization & Regex-based Text Cleaning:**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. Take the original text from I.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. Write a custom tokenization function that:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as it is).   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Use Regex Substitutions (re.sub) to:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) Replace email addresses with `<EMAIL>` placeholder.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) Replace URLs with `<URL>` placeholder.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with `<PHONE>` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0028bce9-92a4-4832-8374-742e520d792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokens: ['Email', 'me', 'at', 'test', 'example', 'com', 'Visit', 'https', 'example', 'com', 'or', 'call', '91', '9876543210']\n",
      "Cleaned Text: Email me at <EMAIL> Visit <URL> or call <PHONE>.\n"
     ]
    }
   ],
   "source": [
    "# Sample text with emails, URLs, and phone numbers\n",
    "text_sample = \"Email me at test@example.com. Visit https://example.com or call +91 9876543210.\"\n",
    "\n",
    "# Custom tokenizer function\n",
    "def custom_tokenizer(text):\n",
    "    # Keep contractions, hyphens, decimal numbers\n",
    "    return re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\", text)\n",
    "\n",
    "# Apply tokenizer\n",
    "tokens = custom_tokenizer(text_sample)\n",
    "\n",
    "# Replace email, URL, phone number with placeholders\n",
    "text_cleaned = re.sub(r'\\S+@\\S+', '<EMAIL>', text_sample)\n",
    "text_cleaned = re.sub(r'https?://\\S+', '<URL>', text_cleaned)\n",
    "text_cleaned = re.sub(r'\\+91\\s?\\d{10}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', text_cleaned)\n",
    "\n",
    "print(\"Custom Tokens:\", tokens)\n",
    "print(\"Cleaned Text:\", text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c7602-3b33-4eae-851a-1667db4f5511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
